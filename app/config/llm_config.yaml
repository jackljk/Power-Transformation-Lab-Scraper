# Config for LLMs

llm:
  # The provider for the LLM
  provider: "openai" # Options: openai, azure, anthropic,, ollama, google, deepseek
  # The LLM model to use
  model: "gpt-4o-mini" 

planner_llm:
  # The provider for the LLM
  provider: "openai" # Options: openai, azure, anthropic,, ollama, google, deepseek
  # The LLM model to use
  model: "o4-mini"

hyperparameters:
  # The temperature for the LLM
  temperature: 1.0
  # The maximum number of tokens to generate
  max_tokens: 2000
  # The top_p for nucleus sampling
  top_p: 1.0
  # The frequency penalty for the LLM
  frequency_penalty: 0.0
  # The presence penalty for the LLM
  presence_penalty: 0.0


# NOTE:
# Deepseek provider is OpenAI